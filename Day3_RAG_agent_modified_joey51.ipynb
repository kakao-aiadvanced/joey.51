{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "의존성 설치"
      ],
      "metadata": {
        "id": "nMTQu3RmPSkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ewS1aTmsOqGo"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install streamlit"
      ],
      "metadata": {
        "id": "UpKl7BFjh-2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOza9gkIjcRG",
        "outputId": "04a14a1a-bc24-412a-df0b-0c2874dc557a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tavily 설정"
      ],
      "metadata": {
        "id": "Z_3RpA83PZTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key='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')\n",
        "\n",
        "# 예시\n",
        "response = tavily.search(query=\"에어컨은 누가 만들었어?\", max_results=3)\n",
        "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
        "\n",
        "context\n"
      ],
      "metadata": {
        "id": "5GJWQyz6PbVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt-4o-mini 준비"
      ],
      "metadata": {
        "id": "XhOTdznQPxc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)"
      ],
      "metadata": {
        "id": "DUAMB2pKP0la"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "벡터저장소에 담을 예제문서"
      ],
      "metadata": {
        "id": "PY2D2oBAP9eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Index\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "# docs>sublist>item\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "m48qiFTBP9QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "관련성 체커"
      ],
      "metadata": {
        "id": "jFeYWEL-Xpu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Relevance Checker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "system = \"\"\"You are a grader assessing relevance\n",
        "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
        "    \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"question: {question}\\n\\n document: {document} \"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()"
      ],
      "metadata": {
        "id": "Yqus4kL3XrWV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm 응답 생성기"
      ],
      "metadata": {
        "id": "oM8F7k-DSQcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "    context를 참고할 땐 그 출처를 꼭 명시해. 출처는 metatdata.source를 확인하면 돼.\n",
        "    \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"question: {question}\\n\\n context: {context} \"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "0KOu3f73SSI-"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "허상 체크"
      ],
      "metadata": {
        "id": "W3HhPYOpSbPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "\n",
        "system = \"\"\"You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
        "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"documents: {documents}\\n\\n answer: {generation} \"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt | llm | JsonOutputParser()\n"
      ],
      "metadata": {
        "id": "UYuvr973Scxa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "엣지 노드 구성"
      ],
      "metadata": {
        "id": "G_waaYUcSz9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "### State\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search: str\n",
        "    documents: List[str]\n",
        "    retry: int\n",
        "\n",
        "\n",
        "### Nodes\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    print(question)\n",
        "    print(documents)\n",
        "    return {\"documents\": documents, \"question\": question, \"retry\": 0}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer using RAG on retrieved documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    retry = state[\"retry\"]\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"retry\": retry + 1}\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based based on the question\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Appended web results to documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    print(state)\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = tavily.search(query=question)['results']\n",
        "    web_results = []\n",
        "    for d in docs:\n",
        "      wd = Document(page_content=d[\"content\"], metadata={\"source\": d[\"url\"]})\n",
        "      web_results.append(wd)\n",
        "\n",
        "    print(\"########## web_results\")\n",
        "    print(web_results)\n",
        "\n",
        "    retry = state[\"retry\"]\n",
        "    return {\"documents\": web_results, \"question\": question, \"retry\": retry + 1}\n",
        "\n",
        "### Conditional edge\n",
        "def relevance_checker(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or add web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---RELEVANCE CHECKER---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    retry = state[\"retry\"]\n",
        "\n",
        "    if retry > 1:\n",
        "      raise ValueError(\"failed: not relevant\")\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "\n",
        "    for d in documents:\n",
        "      result = retrieval_grader.invoke(\n",
        "          {\"question\": question, \"document\": d.page_content}\n",
        "      )\n",
        "      score = result[\"score\"]\n",
        "\n",
        "      if score.lower() == \"yes\":\n",
        "          print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "          filtered_docs.append(d)\n",
        "      else:\n",
        "          print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "\n",
        "    print(\"########### filtered_docs\")\n",
        "    print(filtered_docs)\n",
        "    if len(filtered_docs) > 0:\n",
        "      return \"generate\"\n",
        "    else:\n",
        "      return \"websearch\"\n",
        "\n",
        "def hallucination_checker(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "    retry = state[\"retry\"]\n",
        "\n",
        "    if retry > 2:\n",
        "      raise ValueError(\"failed: hallucination\")\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score[\"score\"]\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "        return \"useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HnNEsMsmS103"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그래프 생성"
      ],
      "metadata": {
        "id": "dax8pU0FT1W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build graph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"websearch\", web_search)  # web search\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    relevance_checker,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"websearch\": \"websearch\"\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"websearch\",\n",
        "    relevance_checker,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"websearch\": \"websearch\"\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    hallucination_checker,\n",
        "    {\n",
        "        \"useful\": END,\n",
        "        \"not supported\": \"generate\"\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnGRVkveT2VM",
        "outputId": "61701de7-a08c-4a9d-ad85-3cc4aade8e4d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b7983033490>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "컴파일 및 실행"
      ],
      "metadata": {
        "id": "ZDG52_jKZssu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# Test\n",
        "\n",
        "inputs = {\"question\": \"카카오 캠퍼스는 어디에있어?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint(f\"Finished running: {key}:\")\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfPEyVrtZuXO",
        "outputId": "b9bb6529-3486-474c-8acd-6157501ee3a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "카카오 캠퍼스는 어디에있어?\n",
            "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'title': \"Prompt Engineering | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\"), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"')]\n",
            "---RELEVANCE CHECKER---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "########### filtered_docs\n",
            "[]\n",
            "'Finished running: retrieve:'\n",
            "---WEB SEARCH---\n",
            "{'question': '카카오 캠퍼스는 어디에있어?', 'documents': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.'), Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.'), Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'title': \"Prompt Engineering | Lil'Log\", 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\"), Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'language': 'en'}, page_content='code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"')], 'retry': 0}\n",
            "########## web_results\n",
            "[Document(metadata={'source': 'https://www.joongang.co.kr/article/25027724'}, page_content='카카오 캠퍼스가 들어서는 곳은 경기도 용인시 낙생저수지 바로 옆이다. 카카오 판교 본사와는 지도 상 직선거리로 6.4㎞, 네이버 그린팩토리와는 3㎞'), Document(metadata={'source': 'https://www.kakaocorp.com/page/detail/10922'}, page_content=\"카카오 (대표이사 홍은택)는 지역 개발자 인재 양성 프로그램인 '카카오테크 캠퍼스' 2기 운영을 위해 부산대학교, 전남대학교, 강원대학교, 경북대학교,\"), Document(metadata={'source': 'https://www.kakaotechcampus.com/'}, page_content='FAQ · 1:1 문의하기. 전체메뉴. 카카오 테크 캠퍼스에서는 함께 배우며 성장합니다. 더 알아보기. 1. 개인정보처리방침. © Kakao Tech Campus. All rights reserved.'), Document(metadata={'source': 'https://www.youtube.com/watch?v=bU4lbqA_RCc'}, page_content='뭔가 다른 지역이나 회사들 보다 인터뷰이들이 여유도 있으시고 대답을 잘 해주는 느낌이네요. 캐치에서 인터뷰했던 다른 지역이나 다른 회사분들은')]\n",
            "---RELEVANCE CHECKER---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "########### filtered_docs\n",
            "[Document(metadata={'source': 'https://www.joongang.co.kr/article/25027724'}, page_content='카카오 캠퍼스가 들어서는 곳은 경기도 용인시 낙생저수지 바로 옆이다. 카카오 판교 본사와는 지도 상 직선거리로 6.4㎞, 네이버 그린팩토리와는 3㎞'), Document(metadata={'source': 'https://www.kakaocorp.com/page/detail/10922'}, page_content=\"카카오 (대표이사 홍은택)는 지역 개발자 인재 양성 프로그램인 '카카오테크 캠퍼스' 2기 운영을 위해 부산대학교, 전남대학교, 강원대학교, 경북대학교,\"), Document(metadata={'source': 'https://www.kakaotechcampus.com/'}, page_content='FAQ · 1:1 문의하기. 전체메뉴. 카카오 테크 캠퍼스에서는 함께 배우며 성장합니다. 더 알아보기. 1. 개인정보처리방침. © Kakao Tech Campus. All rights reserved.')]\n",
            "'Finished running: websearch:'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "'Finished running: generate:'\n",
            "('카카오 캠퍼스는 경기도 용인시 낙생저수지 바로 옆에 위치하고 있습니다. 판교 본사와는 약 6.4㎞ 떨어져 있습니다. (출처: '\n",
            " 'https://www.joongang.co.kr/article/25027724)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYgAOciooynS",
        "outputId": "57e7f2fc-8a26-4cf7-ec1e-d1d880289242"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.139.209.90\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://gentle-bears-share.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08z9ribQs5MC",
        "outputId": "4f459899-25db-4b92-8975-25adb2d5e0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.139.209.90\n",
            "\u001b[1G\u001b[0K⠙\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.139.209.90:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://tidy-forks-juggle.loca.lt\n",
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    }
  ]
}